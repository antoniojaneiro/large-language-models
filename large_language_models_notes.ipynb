{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3011f0-9012-4cde-adbf-7ccd31582965",
   "metadata": {},
   "source": [
    "# Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde31d8-7cce-48eb-a572-7ffc78999790",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models (LLMs)\n",
    "\n",
    "Large Language Models (LLMs) are powerful deep learning models capable of understanding and generating human-like text. They are trained on vast amounts of text data and have demonstrated remarkable performance on a wide range of natural language processing (NLP) tasks.\n",
    "\n",
    "## Content\n",
    "\n",
    "In this notebook, we will:\n",
    "- Explore the basics of LLMs and how they are trained.\n",
    "- Understand the common applications and capabilities of LLMs.\n",
    "- Experiment with using pre-trained models.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Instructions**\n",
    "   - Create account in HuggingFace\n",
    "   - Packages to import\n",
    "2. **Applications of LLMs**\n",
    "   - Text generation, summarization, translation, and more.\n",
    "3. **Getting Started with Hugging Face Transformers**\n",
    "   - Installing the necessary libraries.\n",
    "   - Loading and using a pre-trained model.\n",
    "4. **Sample Task with LLM**\n",
    "   - Choose a task: text generation, sentiment analysis, or question answering.\n",
    "   - Implement the chosen task using a pre-trained model.\n",
    "5. **Future Directions**\n",
    "   - Challenges in LLMs.\n",
    "   - Current research trends and innovations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0b4e2-c78f-42d5-b9e8-0e0962ecdc40",
   "metadata": {},
   "source": [
    "## INSTRUCTIONS\n",
    "\n",
    "1. Create account in [HuggingFace](https://huggingface.co)\n",
    "2. Create token going Settings - > Access Tokens\n",
    "3. Write this in your prompt and insert your token huggingface-cli login\n",
    "4. Have fun!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1642431-b71c-445f-8e7b-d372948254c4",
   "metadata": {},
   "source": [
    "## Packages to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48370b39-fa7c-4d4e-8fc5-057b7110385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoTokenizer, BartForConditionalGeneration, BartTokenizer \n",
    "import sentencepiece as spm\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import evaluate\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899f9b7-07f1-4638-8fdb-ef071173edb0",
   "metadata": {},
   "source": [
    "# Using Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891369e-ba73-47cd-becd-5af5b9edd39d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment Analysis is a common application of Large Language Models (LLMs) where a model assesses the sentiment of a given sentence, typically scoring it on a scale to indicate positivity or negativity. Higher scores represent more positive sentiment, while lower scores indicate negative sentiment.\n",
    "\n",
    "We'll use a pre-trained model from Hugging Face to experiment with sentiment analysis, scoring text from 0 (very negative) to 4 (very positive). This example provides a hands-on look at how LLMs interpret and classify sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d128ea6d-06bb-487b-9de8-deebd4e19240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf31e043-1bb1-4214-a227-d284be202e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return predictions.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d1960d-38e5-45ad-8fa9-b461b9e4249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st test\n",
      "Predicted class for the sentence 'Can we have paella for dinner? I love it!': 4\n",
      "\n",
      "\n",
      "2nd test\n",
      "Predicted class for the sentence 'We have school today?': 2\n",
      "\n",
      "\n",
      "3rd test\n",
      "Predicted class for the sentence 'I don't want to eat spinach': 1\n"
     ]
    }
   ],
   "source": [
    "# Testing the model with examples\n",
    "\n",
    "test_sentence = \"Can we have paella for dinner? I love it!\"\n",
    "predicted_class = classify_sentiment(test_sentence)\n",
    "\n",
    "print(\"1st test\")\n",
    "print(f\"Predicted class for the sentence '{test_sentence}': {predicted_class}\")\n",
    "\n",
    "test_sentence = \"We have school today?\"\n",
    "predicted_class = classify_sentiment(test_sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"2nd test\")\n",
    "print(f\"Predicted class for the sentence '{test_sentence}': {predicted_class}\")\n",
    "\n",
    "test_sentence = \"I don't want to eat spinach\"\n",
    "predicted_class = classify_sentiment(test_sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"3rd test\")\n",
    "print(f\"Predicted class for the sentence '{test_sentence}': {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245cb10-6c9e-4f00-9ff1-96ed2276d737",
   "metadata": {},
   "source": [
    "## Text Summarization\n",
    "\n",
    "Text summarization is a key application of Large Language Models (LLMs) that involves condensing a longer piece of text into a shorter summary while retaining its essential meaning. This technique is particularly useful for extracting key information from articles, reports, and other lengthy documents, allowing users to quickly understand the main points without reading everything.\n",
    "\n",
    "We’ll utilize a pre-trained model from Hugging Face to explore text summarization, transforming extensive content into concise summaries. This example will demonstrate how LLMs can effectively distill information and enhance content accessibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb87ae0-1273-4bf9-acfa-cfb763121bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL TEXT\n",
      "In a small village, a curious boy named Tim loved exploring the woods. \n",
      "One sunny day, he found a hidden path leading to a magical garden. Bright flowers bloomed, and a sparkling pond reflected the sky. \n",
      "As Tim wandered, he met a talking rabbit named Benny. Benny told Tim that the garden was enchanted and only appeared to those with a kind heart. \n",
      "They became friends, sharing stories and laughter. When it was time to leave, Benny gifted Tim a flower that would always remind him of their adventure. \n",
      "Tim promised to return, knowing kindness would lead him back.\n",
      "\n",
      "\n",
      "SUMMARIZED TEXT\n",
      "a curious boy named Tim loves exploring the woods. He finds a hidden path leading to a magical garden. Benny tells Tim that the garden was enchanted and only appeared to those with a kind heart\n"
     ]
    }
   ],
   "source": [
    "# Text to summarize\n",
    "\n",
    "long_text = \"\"\"In a small village, a curious boy named Tim loved exploring the woods. \n",
    "One sunny day, he found a hidden path leading to a magical garden. Bright flowers bloomed, and a sparkling pond reflected the sky. \n",
    "As Tim wandered, he met a talking rabbit named Benny. Benny told Tim that the garden was enchanted and only appeared to those with a kind heart. \n",
    "They became friends, sharing stories and laughter. When it was time to leave, Benny gifted Tim a flower that would always remind him of their adventure. \n",
    "Tim promised to return, knowing kindness would lead him back.\"\"\"\n",
    "\n",
    "\n",
    "model_name = \"cnicu/t5-small-booksum\"\n",
    "\n",
    "# Load model\n",
    "summarizer = pipeline(task=\"summarization\", model=model_name)\n",
    "\n",
    "outputs = summarizer(long_text, max_length=50)\n",
    "print(\"FULL TEXT\")\n",
    "print(long_text)\n",
    "print(\"\\n\")\n",
    "print(\"SUMMARIZED TEXT\")\n",
    "print(outputs[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542c976-0aef-4d12-a9c3-924c41bac6dd",
   "metadata": {},
   "source": [
    "We can observe that the summary was ok but could be improved. We will come back to text summarization and apply some techniques to make the summarized text cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf2f0b-8369-4b81-8c15-2ab9da42c87c",
   "metadata": {},
   "source": [
    "## Question & Answering\n",
    "\n",
    "Question Answering (QA) is a popular application of Large Language Models (LLMs) that enables systems to provide precise answers to questions based on a given context. In this task, a model analyzes the input text to locate relevant information and extract answers, making it an essential tool for various applications such as search engines, customer support, and educational platforms.\n",
    "\n",
    "We will not be providing a model to experiment and observe how, when we don't provide a model the pipeline selects a default model given the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd9085f-20b9-447a-8518-1e2e44084390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "qa_model = pipeline(task=\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54212e65-033d-44c1-a286-a8c0ad8f702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mercury, Venus, Earth, and Mars\n"
     ]
    }
   ],
   "source": [
    "# Context for the model to learn from to be able to answer our question\n",
    "\n",
    "context = \"\"\"\n",
    "The solar system is a vast and complex structure that consists of the Sun and various celestial bodies that orbit it. \n",
    "At the center is the Sun, a star that provides the necessary light and heat to sustain life on Earth. \n",
    "There are eight planets in our solar system, which are categorized into two groups: terrestrial planets and gas giants. \n",
    "The terrestrial planets, which are closer to the Sun, include Mercury, Venus, Earth, and Mars. \n",
    "These planets are primarily composed of rock and metal and have solid surfaces, allowing for geological processes. \n",
    "Mercury is the smallest planet and closest to the Sun, while Venus is known for its thick atmosphere and high temperatures. \n",
    "Earth is unique for its ability to support life, and Mars is often studied for its potential to harbor past life. \n",
    "\n",
    "In contrast, the gas giants—Jupiter, Saturn, Uranus, and Neptune—are located farther from the Sun. \n",
    "These planets are composed mainly of hydrogen and helium and do not have well-defined solid surfaces. \n",
    "Jupiter is the largest planet in our solar system and is known for its Great Red Spot, a giant storm. \n",
    "Saturn is famous for its stunning rings, while Uranus and Neptune are noted for their bluish hues due to methane in their atmospheres. \n",
    "\n",
    "Additionally, there are dwarf planets like Pluto, which, although no longer classified as a major planet, plays a significant role in our understanding of the solar system's formation. \n",
    "Numerous moons, comets, and asteroids further contribute to the complexity and diversity of our solar system, making it a fascinating subject for scientific exploration and study.\n",
    "\"\"\"\n",
    "\n",
    "# Example question\n",
    "question = \"What are the four terrestrial planets in our solar system?\"\n",
    "\n",
    "\n",
    "outputs = qa_model(question=question, context = context)\n",
    "print(outputs['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83d885-c557-47e0-a929-c47c89e162b1",
   "metadata": {},
   "source": [
    "# Text Translation\n",
    "\n",
    "Text Translation is a fascinating application of Large Language Models (LLMs) that enables the conversion of text from one language to another. This process involves understanding the meaning of the original text and accurately generating the equivalent text in the target language, maintaining both context and nuance. \n",
    "\n",
    "We'll leverage a pre-trained model from Hugging Face to experiment with text translation, allowing us to translate sentences from Spanish to English seamlessly. This hands-on example illustrates how LLMs can bridge language barriers and facilitate communication across different cultures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a4cf01-632c-4963-bcf0-eb75657c63e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AJ\\large-language-models\\llm_env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text in Spanish:\n",
      "Me encanta jugar al futbol! Mi jugador favorito es Cristiano Ronaldo!\n",
      "\n",
      "\n",
      "Text after translation to English:\n",
      "I love playing football! My favorite player is Cristiano Ronaldo!\n"
     ]
    }
   ],
   "source": [
    "# Text to translate\n",
    "input_text = \"Me encanta jugar al futbol! Mi jugador favorito es Cristiano Ronaldo!\"\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a translation pipeline\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "output = translator(input_text)\n",
    "\n",
    "print('Text in Spanish:')\n",
    "print(input_text)\n",
    "print('\\n')\n",
    "print('Text after translation to English:')\n",
    "print(output[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d7125-f97d-412d-9de6-2060d0d63196",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "Text Generation is a compelling application of Large Language Models (LLMs) that enables the creation of coherent and contextually relevant text based on a given prompt. By analyzing the input text, LLMs generate responses that can range from simple sentences to complex paragraphs, making them useful for various applications like storytelling, content creation, and conversational agents.\n",
    "\n",
    "In this example, we'll utilize a pre-trained model from Hugging Face to explore text generation, allowing us to generate creative and informative responses to specific prompts. This hands-on exercise showcases the remarkable capabilities of LLMs in producing human-like text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129b35c9-5a6f-485e-bec7-2e1056cc500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "generator = pipeline(task=\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87deec30-1bd8-4e75-a75c-b50c893cc1e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer review:\n",
      "\n",
      "I recently stayed at the Grand Vista Hotel for a weekend getaway, and it was an amazing experience! \n",
      "The check-in process was smooth and the staff were incredibly friendly and accommodating. \n",
      "My room was spacious, clean, and beautifully decorated, with a stunning view of the city skyline. \n",
      "I particularly enjoyed the hotel amenities, including the rooftop pool and the complimentary breakfast that offered a wide variety of delicious options. \n",
      "The location was perfect, within walking distance of popular attractions and great restaurants. \n",
      "Overall, I had a wonderful stay and would definitely recommend the Grand Vista Hotel to anyone visiting the area!\n",
      "\n",
      "\n",
      "Hotel response to the customer:\n",
      "Dear valued customer, I am glad to hear you had a good stay with us.  Unfortunately, during the coming days of business, we ran a 2 week queue for reservations and a 3 week waiting period in some places, so that we can get a good deal after the hotel closes. Therefore, we canceled all of the reservations until we were able to pay.  At that point, we had to wait for an additional month or so until the full list was available.  It would have been great to have some time off from work so we could prepare for this visit when the hotel opens today.  I would strongly recommend that you wait, otherwise we could have simply called. For me, I love how they are always available to accommodate new customers even after closing time.  I\n"
     ]
    }
   ],
   "source": [
    "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
    "\n",
    "customer_review = \"\"\"\n",
    "I recently stayed at the Grand Vista Hotel for a weekend getaway, and it was an amazing experience! \n",
    "The check-in process was smooth and the staff were incredibly friendly and accommodating. \n",
    "My room was spacious, clean, and beautifully decorated, with a stunning view of the city skyline. \n",
    "I particularly enjoyed the hotel amenities, including the rooftop pool and the complimentary breakfast that offered a wide variety of delicious options. \n",
    "The location was perfect, within walking distance of popular attractions and great restaurants. \n",
    "Overall, I had a wonderful stay and would definitely recommend the Grand Vista Hotel to anyone visiting the area!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Customer review:\\n{customer_review}\\n\\nHotel response to the customer:\\n{response}\"\n",
    "\n",
    "outputs = generator(prompt, max_length=300, truncation=True, pad_token_id= generator.tokenizer.eos_token_id)\n",
    "\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cae631-8e3f-411e-8a12-1cfa835ca7cf",
   "metadata": {},
   "source": [
    "The results may not be optimal, but there's no need to worry! We're currently testing basic models. Later in the notebook, we'll go deeper into text-generation techniques to improve our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308440f6-b0c2-4de1-9296-496c6aa07568",
   "metadata": {},
   "source": [
    "# Understanding Transformer Architecture\n",
    "\n",
    "## What is a Transformer?\n",
    "\n",
    "A **Transformer** is a deep learning model architecture introduced in the paper *\"Attention is All You Need\"* by Vaswani et al. (2017). This architecture has become the foundation for many state-of-the-art models in natural language processing (NLP). The Transformer leverages a mechanism known as **self-attention**, enabling the model to evaluate the importance of different words in a sequence when making predictions.\n",
    "\n",
    "## Transformer Architecture\n",
    "\n",
    "The Transformer consists of an **encoder-decoder** structure:\n",
    "\n",
    "- **Encoder**: The encoder processes an input sequence to create a set of contextualized embeddings. Each encoder layer includes a multi-head self-attention mechanism and a feedforward neural network.\n",
    "\n",
    "- **Decoder**: The decoder generates the output sequence based on the encoder's output and previously generated tokens. It features multiple layers, which include masked multi-head self-attention and encoder-decoder attention.\n",
    "\n",
    "### Evolving Transformer Architectures\n",
    "\n",
    "While the original Transformer used both an encoder and a decoder, modern implementations often adopt variations based on task requirements:\n",
    "\n",
    "1. **Encoder-Only Models**: Designed for tasks like text classification and named entity recognition, where only the input context is relevant.  \n",
    "   *Examples*: BERT, RoBERTa, DistilBERT.\n",
    "\n",
    "2. **Decoder-Only Models**: Used for tasks like text generation, where previously generated tokens are needed to predict the next token.  \n",
    "   *Examples*: GPT, GPT-2, GPT-3.\n",
    "\n",
    "3. **Encoder-Decoder Models**: Suitable for tasks like text translation, where both input and output sequences are essential.  \n",
    "   *Examples*: T5, BART.\n",
    "\n",
    "## Detailed Explanation of the Transformer Encoder Structure\n",
    "\n",
    "### Overview of the Encoder\n",
    "\n",
    "The encoder processes input sequences to create contextualized embeddings. It plays a crucial role in understanding the relationships between words and capturing the nuances of the input data. The encoder transforms input tokens into rich representations that can be utilized by the decoder or for various downstream tasks.\n",
    "\n",
    "### Structure of the Encoder\n",
    "\n",
    "The Transformer encoder consists of multiple layers, each containing several key components:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "2. **Feedforward Neural Network**\n",
    "3. **Add & Norm**\n",
    "\n",
    "Let’s explore each of these components in detail:\n",
    "\n",
    "### 1. Multi-Head Self-Attention\n",
    "\n",
    "- **Purpose**:  \n",
    "  This mechanism allows the encoder to assess the importance of different words relative to each other, capturing dependencies and relationships regardless of their positions in the sequence.\n",
    "\n",
    "- **How It Works**:\n",
    "  - **Input**: The encoder takes input tokens.\n",
    "  - **Self-Attention Calculation**: The self-attention mechanism computes attention scores for each token, determining how much focus each token should receive based on others.\n",
    "  - **Multi-Head Mechanism**: The model uses multiple attention heads to learn various types of relationships, each capturing different aspects of the input sequence.\n",
    "\n",
    "### 2. Feedforward Neural Network\n",
    "\n",
    "- **Purpose**:  \n",
    "  Each position in the sequence is processed independently through a feedforward neural network, introducing non-linearity and enhancing the model's capacity to learn complex representations.\n",
    "\n",
    "- **How It Works**:\n",
    "  - **Input**: The output from the multi-head attention component.\n",
    "  - **Structure**: Typically consists of two linear transformations with a ReLU activation function in between.\n",
    "  - **Output**: The output is a transformed version of the input, capturing intricate relationships within the token embeddings.\n",
    "\n",
    "### 3. Add & Norm\n",
    "\n",
    "- **Purpose**:  \n",
    "  This step adds the input from the previous layer to the output of the current layer and normalizes the result, stabilizing the training process and improving gradient flow.\n",
    "\n",
    "- **How It Works**:\n",
    "  - **Add**: The input from the previous layer is added to the current layer's output, forming a residual connection to help mitigate the vanishing gradient problem.\n",
    "  - **Layer Normalization**: The result is normalized, enhancing convergence during training.\n",
    "\n",
    "### Stacking Layers\n",
    "\n",
    "Each of these components is stacked to form multiple encoder layers. Typically, Transformers have an equal number of encoder and decoder layers, though this is not a strict rule. The stack depth can be adjusted based on task complexity and model size.\n",
    "\n",
    "### Output of the Encoder\n",
    "\n",
    "After passing through all encoder layers, the final output consists of contextualized embeddings for each input token. These embeddings can then be utilized by the decoder or for various downstream tasks, such as text classification or sentiment analysis.\n",
    "\n",
    "## Detailed Explanation of the Transformer Decoder Structure\n",
    "\n",
    "### Overview of the Decoder\n",
    "\n",
    "The decoder generates output sequences from the encoded representations provided by the encoder. It is particularly crucial for tasks like machine translation, text summarization, and any other text generation tasks. The decoder predicts the next token in a sequence based on previously generated tokens and the encoder’s output.\n",
    "\n",
    "### Structure of the Decoder\n",
    "\n",
    "The Transformer decoder consists of several layers (typically the same number as the encoder) stacked on top of each other. Each layer contains several key components:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "2. **Multi-Head Attention**\n",
    "3. **Feedforward Neural Network**\n",
    "4. **Add & Norm**\n",
    "\n",
    "Let’s delve deeper into each of these components:\n",
    "\n",
    "### 1. Masked Multi-Head Self-Attention\n",
    "\n",
    "- **Purpose**:  \n",
    "  This mechanism allows the decoder to attend to previously generated tokens while generating the next token in the sequence. Masking ensures that the model cannot \"cheat\" by looking at future tokens.\n",
    "\n",
    "- **How It Works**:\n",
    "  - **Input**: The decoder receives previous outputs (tokens) as input.\n",
    "  - **Self-Attention Calculation**: The self-attention computes attention scores for each token, determining how much focus to place on other tokens when producing the next token.\n",
    "  - **Masking**: A mask is applied to ensure the model only attends to tokens that have already been generated, preventing access to later tokens in the sequence.\n",
    "\n",
    "- **Multi-Head Mechanism**:  \n",
    "  This aspect allows the model to capture different relationships and dependencies between tokens using multiple attention heads, each processing input independently.\n",
    "\n",
    "### 2. Multi-Head Attention (Encoder-Decoder Attention)\n",
    "\n",
    "- **Purpose**:  \n",
    "  This component allows the decoder to attend to the encoder’s output, leveraging contextual embeddings to generate more informed and contextually relevant output.\n",
    "\n",
    "- **How It Works**:\n",
    "  - **Input**: The output from the encoder and the decoder’s previous layer (after masked self-attention).\n",
    "  - **Attention Calculation**: The decoder computes attention scores between the current output and all encoder outputs, focusing on relevant parts of the input sequence based on what it is currently generating.\n",
    "  - **Output**: The results are combined and passed to the next layer.\n",
    "\n",
    "### 3. Feedforward Neural Network\n",
    "\n",
    "- **Purpose**:  \n",
    "  Each position in the sequence is processed independently through a feedforward neural network, introducing non-linearity to the model.\n",
    "\n",
    "- **How It Works**:\n",
    "  - **Input**: The output from the multi-head attention component.\n",
    "  - **Structure**: Typically consists of two linear transformations with a ReLU activation function in between.\n",
    "  - **Output**: The output is a transformed version of the input that captures complex interactions between tokens.\n",
    "\n",
    "### 4. Add & Norm\n",
    "\n",
    "- **Purpose**:  \n",
    "  This step adds the input from the previous layer to the output of the current layer and normalizes the result, stabilizing the training process and allowing for better gradient flow.\n",
    "\n",
    "- **How It Works**:\n",
    "  - **Add**: The input to the current layer (from the previous layer) is added to its output, forming a residual connection to prevent the vanishing gradient problem.\n",
    "  - **Layer Normalization**: The result is normalized, improving convergence during training.\n",
    "\n",
    "### Stacking Layers\n",
    "\n",
    "Each of these components is stacked to form multiple decoder layers. Typically, Transformers have an equal number of encoder and decoder layers, but this is not a strict rule. The depth of the stack can be adjusted based on task complexity and model size.\n",
    "\n",
    "### Final Linear Layer and Softmax\n",
    "\n",
    "After passing through all decoder layers, the final output is sent to a linear layer that projects it into the size of the vocabulary. This is followed by a softmax activation function to produce a probability distribution over the vocabulary, allowing the model to select the most likely next token in the sequence.\n",
    "\n",
    "### Transformer Architecture: Encoder-Decoder\n",
    "\n",
    "Initially, Transformers were designed as **encoder-decoder** architectures, suitable for tasks requiring mapping one sequence to another, such as machine translation.\n",
    "\n",
    "- **Encoder**:\n",
    "  - Processes the input sequence and creates context-aware representations for each token, utilizing multiple layers that apply self-attention to understand dependencies in the input.\n",
    "\n",
    "- **Decoder**:\n",
    "  - Generates the output sequence one token at a time, attending to both the encoder's output and the previously generated tokens to ensure coherence in the output.\n",
    "\n",
    "## Modern Variations of Transformer Architectures\n",
    "\n",
    "While the encoder-decoder architecture is still widely used, many applications can benefit from using either only the encoder or only the decoder, allowing for more flexibility and efficiency.\n",
    "\n",
    "### 1. Encoder-Only Models\n",
    "\n",
    "#### Overview\n",
    "Encoder-only models focus on understanding and representing input text, generating contextual embeddings for each token without performing text generation.\n",
    "\n",
    "#### Common Use Cases\n",
    "- **Text Classification**: Categorizing text into predefined classes.\n",
    "- **Named Entity Recognition**: Identifying and classifying entities in text.\n",
    "\n",
    "**Examples of Encoder-Only Models**:\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Designed for context-based tasks like sentiment analysis and question answering.\n",
    "- **RoBERTa**: An improved version of BERT that utilizes more data and longer training times for better performance.\n",
    "- **DistilBERT**: A smaller, faster, and lighter variant of BERT that maintains a significant portion of its performance.\n",
    "\n",
    "### 2. Decoder-Only Models\n",
    "\n",
    "#### Overview\n",
    "Decoder-only models are tailored for generating text, predicting the next token in a sequence based solely on previous tokens.\n",
    "\n",
    "#### Common Use Cases\n",
    "- **Text Generation**: Producing coherent and contextually relevant text based on a given prompt.\n",
    "- **Dialogue Systems**: Creating conversational agents that respond dynamically.\n",
    "\n",
    "**Examples of Decoder-Only Models**:\n",
    "- **GPT (Generative Pre-trained Transformer)**: Designed for generative tasks, allowing it to produce text that follows a given prompt.\n",
    "- **GPT-2 and GPT-3**: Extensions of GPT that are larger in size, leading to more sophisticated text generation capabilities.\n",
    "\n",
    "### 3. Encoder-Decoder Models\n",
    "\n",
    "#### Overview\n",
    "Encoder-decoder models leverage both components to process an input sequence and generate an output sequence. They are particularly useful for tasks requiring a mapping from one text to another.\n",
    "\n",
    "#### Common Use Cases\n",
    "- **Machine Translation**: Translating text from one language to another.\n",
    "- **Text Summarization**: Condensing long articles into concise summaries.\n",
    "\n",
    "**Examples of Encoder-Decoder Models**:\n",
    "- **T5 (Text-to-Text Transfer Transformer)**: Treats all NLP tasks as text-to-text problems, allowing it to handle various tasks, including translation and summarization.\n",
    "- **BART (Bidirectional and Auto-Regressive Transformers)**: Combines the bidirectional nature of BERT with the generative capabilities of GPT, making it effective for tasks like summarization and text generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbec050-437e-44db-84ba-8f1be38502bd",
   "metadata": {},
   "source": [
    "## Building a Transformer\n",
    "PyTorch's nn.Transformer class provides a full transformer architecture with pre-built encoder and decoder stacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fff137-d7bc-4099-98c6-781db2809a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the model dimension\n",
    "# The model dimension (d_model) is a fundamental hyperparameter that influences both the expressiveness and efficiency of the transformer model. \n",
    "# When designing or tuning a transformer model, it's important to balance the model dimension with \n",
    "# available resources and the complexity of the tasks being addressed.\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "# Define the number of attention heads\n",
    "n_heads = 8\n",
    "\n",
    "# Define the number of encoder layers\n",
    "num_encoder_layers = 6\n",
    "\n",
    "# Define the number of decoder layers\n",
    "num_decoder_layers = 6\n",
    "\n",
    "model = nn.Transformer(\n",
    "    d_model = d_model,\n",
    "    nhead = n_heads,\n",
    "    num_encoder_layers = num_encoder_layers,\n",
    "    num_decoder_layers = num_decoder_layers,\n",
    "    batch_first=True,  # Set to True for better performance with batch dimension first\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08175a-038b-4d82-994d-7b9ae17b23ff",
   "metadata": {},
   "source": [
    "# Attention Mechanism and Positional Encoding\n",
    "\n",
    "## Attention Mechanism\n",
    "\n",
    "The attention mechanism is a key component of transformer models that allows them to determine the importance of different words in a sentence when making predictions. It enables the model to focus on specific parts of the input sequence, regardless of their position, enhancing its understanding of context.\n",
    "\n",
    "### How Attention Works\n",
    "\n",
    "In the attention mechanism, each word in the input is transformed into three types of vectors:\n",
    "\n",
    "- **Query**: Represents the word we are focusing on.\n",
    "- **Key**: Represents other words that could be relevant.\n",
    "- **Value**: Contains the actual information we want to retrieve.\n",
    "\n",
    "The attention process involves the following steps:\n",
    "\n",
    "1. **Score Calculation**: The model computes a score for each word by comparing the query with all the keys. This score indicates how relevant each key is to the query.\n",
    "2. **Weighting**: The scores are normalized to create weights. Higher weights indicate that the corresponding word is more relevant to the query.\n",
    "3. **Combining Values**: The model then takes a weighted sum of the value vectors based on the computed weights. This results in an output that emphasizes the most relevant information.\n",
    "\n",
    "### Types of Attention\n",
    "\n",
    "- **Self-Attention**: This type of attention evaluates how each word relates to every other word in the same sequence, allowing the model to capture contextual relationships within the input.\n",
    "- **Cross-Attention**: This type of attention is used in models that compare two different sequences, such as in translation tasks, where the input may come from one language and the output in another.\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "Transformers do not inherently understand the order of tokens because they do not use recurrence. To solve this, positional encoding is added to the input embeddings, providing essential information about the position of each token in the sequence.\n",
    "\n",
    "### How Positional Encoding Works\n",
    "\n",
    "Positional encodings are unique vectors assigned to each token based on its position in the sequence. These encodings are combined with the token embeddings, allowing the model to recognize the order of words. \n",
    "\n",
    "The positional encodings use a pattern based on sine and cosine functions. This pattern ensures that tokens at different positions have distinct encodings while maintaining a relationship between them. For example, nearby tokens will have similar positional encodings, which helps the model understand their proximity in context.\n",
    "\n",
    "### Importance of Positional Encoding\n",
    "\n",
    "- **Order Information**: By adding positional encodings, the model gains an understanding of the sequence's structure, allowing it to interpret the meaning more accurately.\n",
    "- **Context Awareness**: Positional encodings help the model maintain context by recognizing how words relate to each other based on their positions.\n",
    "\n",
    "Combining the attention mechanism with positional encodings enables transformer models to process sequential data effectively while maintaining a strong contextual understanding of the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a2f8a2-6efb-4409-93ce-6ed38cfe89e6",
   "metadata": {},
   "source": [
    "# Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20f0db9e-4f88-4269-8385-bf0d5e725110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_length = max_length\n",
    "        # Initialize the positional encoding matrix\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (math.log(10000)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # o 2 era 3 antes\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a73da-1ef4-4b63-a2ab-f84c233f8699",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f5a870-6f68-4b58-bd07-fcc1957f7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_module, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Set number of attention heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "        # Set up the linear transformations\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Split the sequence embeddings in x across the attention heads\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n",
    "\n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        # Compute dot-product attention scores\n",
    "        scores = torch.matmul(query, key.permute(0, 2, 1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float(\"-1e20\"))\n",
    "        # Normalize attention scores into attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask= None):\n",
    "        batch_size = query.size(0) # era 8 antes\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "        # Multiply attention weights by values, concatenate and linearly outputs\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d5e3d-a442-453a-9c6e-469a904114a9",
   "metadata": {},
   "source": [
    "# Encoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a700900-3616-4076-b8cb-cd61515d3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    " \n",
    "\n",
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        return self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    " \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d72dfb11-f619-4923-86c7-ba5e94ddccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification outputs for a batch of 8 sequences:\n",
      "tensor([[-1.2599, -1.7033, -0.6269],\n",
      "        [-1.6379, -0.9813, -0.8421],\n",
      "        [-1.4597, -0.5489, -1.6602],\n",
      "        [-1.0894, -1.4754, -0.8327],\n",
      "        [-1.4466, -0.9730, -0.9502],\n",
      "        [-1.2586, -1.8287, -0.5882],\n",
      "        [-0.9719, -0.9390, -1.4669],\n",
      "        [-1.1809, -0.7247, -1.5678]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing the encoder transformer\n",
    "\n",
    "# Hyperparameters for the transformer model\n",
    "num_classes = 3            # Number of output classes for classification\n",
    "vocab_size = 10000         # Size of the vocabulary\n",
    "batch_size = 8             # Number of sequences to process in a batch\n",
    "d_model = 512              # Dimension of the model (embeddings and hidden states)\n",
    "num_heads = 8              # Number of attention heads in multi-head attention\n",
    "num_layers = 6             # Number of encoder layers in the transformer\n",
    "d_ff = 2048                # Dimension of the feedforward network\n",
    "sequence_length = 256      # Maximum length of input sequences\n",
    "dropout = 0.1              # Dropout rate for regularization\n",
    "\n",
    "# Generate random input sequences (batch_size x sequence_length)\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Create a random attention mask (sequence_length x sequence_length)\n",
    "# This mask is used to prevent attending to certain positions in the sequence\n",
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Instantiate the encoder transformer's body and the classification head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Perform a forward pass through the encoder\n",
    "output = encoder(input_sequence, mask)\n",
    "\n",
    "# Pass the encoder's output through the classifier head to get class predictions\n",
    "classification = classifier(output)\n",
    "\n",
    "# Print the classification outputs for the current batch of sequences\n",
    "print(\"Classification outputs for a batch of\", batch_size, \"sequences:\")\n",
    "print(classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560a41a-6945-4760-a808-3adf40244b90",
   "metadata": {},
   "source": [
    "Each row in the output represents a sequence, displaying the log probabilities for each class. The model predicts the class with the highest log probability for each sequence.\n",
    "\n",
    "Overall, this output reflects the model’s confidence in its predictions, indicating the most likely class for each sequence based on the provided log probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36892b40-cfdc-498f-a48b-43f3d7bfcfbf",
   "metadata": {},
   "source": [
    "# Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d72dd7e-b931-4763-b17c-9b9ebb2f6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ca4526f-1329-4712-b5d1-39d1a8537186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask, encoder_output, cross_mask)\n",
    "        return self.classifier(x)  # Returns logits for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74edb8-bc50-421e-8703-73bf33da4148",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ca3a05-9eac-4d89-acab-c6609ab7db4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " tensor([[9182, 5134, 2795,  ..., 2877, 1863, 7910],\n",
      "        [2538, 6845, 5967,  ..., 4464,  938, 5013],\n",
      "        [6363, 1339,  385,  ..., 7246, 4582, 3041],\n",
      "        ...,\n",
      "        [5635, 4395, 4278,  ..., 1486, 5838, 7712],\n",
      "        [5388, 9552,  606,  ..., 2421,  767, 4178],\n",
      "        [ 187, 1044, 6659,  ..., 1696, 8874, 7728]])\n",
      "Encoder output shape: torch.Size([8, 256, 512])\n",
      "Encoder output:\n",
      " tensor([[[-7.2958e-01, -9.3359e-01,  1.0381e+00,  ...,  4.7121e-01,\n",
      "           4.9682e-01,  3.1380e-01],\n",
      "         [-8.5936e-02,  1.2037e+00, -4.9680e-01,  ..., -4.9237e-01,\n",
      "          -3.5736e-01, -3.7516e-01],\n",
      "         [ 1.2578e+00,  2.3459e-01,  1.5557e+00,  ..., -2.5342e-01,\n",
      "          -1.4801e+00, -1.0275e+00],\n",
      "         ...,\n",
      "         [ 1.4393e+00,  8.5162e-01, -2.0367e+00,  ..., -5.0844e-01,\n",
      "          -5.3889e-01,  5.9611e-01],\n",
      "         [-5.1589e-01, -4.2206e-01, -4.5397e-01,  ..., -9.6882e-01,\n",
      "          -1.6140e+00,  1.4337e+00],\n",
      "         [-3.7493e-01, -3.8852e-01,  1.5543e+00,  ..., -6.9486e-01,\n",
      "          -3.5260e-01,  2.6178e+00]],\n",
      "\n",
      "        [[-9.4001e-01, -2.2203e-03, -2.4384e-01,  ..., -1.7439e+00,\n",
      "           1.6202e-01,  3.4072e-01],\n",
      "         [-9.2734e-03,  1.2436e+00,  8.0652e-01,  ...,  1.3032e+00,\n",
      "           1.7372e-02, -9.8005e-01],\n",
      "         [ 1.4535e+00, -7.9905e-02,  1.8348e+00,  ...,  2.9075e-01,\n",
      "          -4.6945e-01, -2.9635e-02],\n",
      "         ...,\n",
      "         [-4.9924e-01, -1.2685e+00, -4.1049e-01,  ...,  8.1482e-01,\n",
      "          -1.5033e-01, -1.3500e-01],\n",
      "         [ 9.6703e-02, -8.8083e-01,  5.7500e-01,  ...,  1.8313e-01,\n",
      "          -2.7779e-01,  8.7451e-01],\n",
      "         [-6.0290e-02, -8.0289e-01,  1.1346e+00,  ...,  3.0353e-01,\n",
      "           1.2349e+00,  1.8301e+00]],\n",
      "\n",
      "        [[ 1.2610e+00,  1.8121e+00, -5.2113e-01,  ..., -5.5450e-01,\n",
      "          -1.2604e-01,  1.8959e+00],\n",
      "         [ 6.4406e-01, -2.0494e-04,  1.2584e+00,  ..., -1.2398e+00,\n",
      "           1.1587e+00,  4.0521e-01],\n",
      "         [ 8.0264e-01,  7.3206e-01, -6.9821e-01,  ...,  1.0398e+00,\n",
      "          -1.2826e+00, -4.3281e-01],\n",
      "         ...,\n",
      "         [-1.7507e-01, -8.4315e-01, -3.5353e-01,  ...,  1.0151e+00,\n",
      "           1.1901e+00,  4.2665e-02],\n",
      "         [-1.0989e+00, -1.4412e-01, -1.4226e+00,  ..., -1.0807e+00,\n",
      "          -1.1812e+00,  6.1011e-01],\n",
      "         [-6.0683e-01, -6.1414e-01,  8.8103e-01,  ...,  6.7276e-01,\n",
      "          -4.5721e-01,  1.0943e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5841e+00,  7.0717e-01, -1.4469e-02,  ..., -5.1674e-01,\n",
      "          -1.3463e+00,  5.9989e-01],\n",
      "         [ 3.6617e-02,  2.0948e+00,  7.2847e-01,  ...,  1.1870e+00,\n",
      "           1.2869e+00, -6.2269e-01],\n",
      "         [ 5.6403e-01, -2.2040e-02,  7.4931e-01,  ...,  1.1726e+00,\n",
      "          -4.6282e-01, -1.5335e-01],\n",
      "         ...,\n",
      "         [ 1.2992e+00,  1.1580e+00, -8.9726e-01,  ...,  1.4442e+00,\n",
      "           9.3433e-01, -7.4935e-01],\n",
      "         [-1.7093e-01, -8.4813e-01,  1.0427e-01,  ...,  7.2804e-01,\n",
      "          -4.0343e-01,  1.2726e+00],\n",
      "         [ 4.8120e-02,  2.0098e-01, -9.4926e-01,  ...,  2.9864e-01,\n",
      "           6.9864e-01,  1.9632e+00]],\n",
      "\n",
      "        [[-2.7292e-01,  6.6764e-01, -4.0407e-01,  ...,  2.5425e-01,\n",
      "          -1.5354e+00,  1.4804e+00],\n",
      "         [ 9.1012e-01,  1.5213e+00,  2.4788e+00,  ...,  6.7283e-01,\n",
      "          -7.4828e-02, -4.8623e-01],\n",
      "         [ 2.8131e-01,  1.6662e+00, -1.0411e-01,  ...,  4.1584e-01,\n",
      "           5.7857e-01,  3.3154e-01],\n",
      "         ...,\n",
      "         [ 1.5960e+00, -6.7174e-01, -6.6644e-01,  ...,  9.8912e-01,\n",
      "           5.3670e-01, -1.0498e+00],\n",
      "         [ 5.3015e-01, -1.1177e+00, -1.9825e-01,  ...,  1.1621e+00,\n",
      "          -7.3099e-01,  7.4779e-01],\n",
      "         [-5.4625e-01, -5.2259e-01,  3.1060e-01,  ...,  8.7941e-01,\n",
      "           1.6713e-01,  3.7224e-01]],\n",
      "\n",
      "        [[ 5.2970e-01,  1.0949e+00, -2.8161e-02,  ..., -4.1072e-01,\n",
      "           9.5152e-01, -3.9592e-01],\n",
      "         [-1.4820e-01,  7.3140e-01,  4.4149e-01,  ...,  1.0644e+00,\n",
      "           1.2854e+00, -1.4061e+00],\n",
      "         [ 2.3014e+00,  8.7338e-01,  2.3356e-01,  ...,  4.5921e-01,\n",
      "           2.7697e-01, -8.1203e-01],\n",
      "         ...,\n",
      "         [ 1.0019e+00, -4.1770e-01, -1.0362e+00,  ...,  5.2445e-01,\n",
      "          -9.9540e-01,  9.9080e-01],\n",
      "         [ 1.9268e+00, -9.6881e-01, -1.9294e+00,  ..., -1.2645e+00,\n",
      "          -7.1433e-01,  1.8388e+00],\n",
      "         [ 1.0032e-01, -7.8600e-02,  9.0215e-01,  ..., -4.5961e-01,\n",
      "           2.4927e+00,  8.3403e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "Batch's output shape: torch.Size([8, 3])\n",
      "Decoder output:\n",
      " tensor([[-0.8463, -1.6920, -0.9497],\n",
      "        [-0.9634, -1.7140, -0.8250],\n",
      "        [-1.3365, -1.0980, -0.9071],\n",
      "        [-0.5382, -1.2972, -1.9455],\n",
      "        [-1.1609, -1.4270, -0.8057],\n",
      "        [-1.6036, -1.1674, -0.7182],\n",
      "        [-1.4688, -1.6120, -0.5616],\n",
      "        [-1.2818, -2.0995, -0.5109]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing the Decoder Transformer\n",
    "\n",
    "# Hyperparameters for the transformer model\n",
    "num_classes = 3          # Number of output classes\n",
    "vocab_size = 10000       # Size of the vocabulary\n",
    "batch_size = 8           # Number of sequences in a batch\n",
    "d_model = 512            # Dimensionality of the model's output space\n",
    "num_heads = 8            # Number of attention heads in the multi-head attention mechanism\n",
    "num_layers = 6           # Number of layers in the encoder and decoder\n",
    "d_ff = 2048              # Dimensionality of the feedforward layer\n",
    "sequence_length = 256    # Length of input sequences\n",
    "dropout = 0.1            # Dropout rate for regularization\n",
    "\n",
    "# Create a batch of random input sequences (tokenized)\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Create a padding mask to ignore padding tokens in sequences\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Create a causal mask to prevent the decoder from attending to future tokens\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the transformer encoder\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size, \n",
    "    d_model, \n",
    "    num_layers, \n",
    "    num_heads, \n",
    "    d_ff, \n",
    "    dropout, \n",
    "    max_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Instantiate the transformer decoder\n",
    "decoder = TransformerDecoder(\n",
    "    vocab_size, \n",
    "    d_model, \n",
    "    num_layers, \n",
    "    num_heads, \n",
    "    d_ff, \n",
    "    dropout, \n",
    "    max_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "# Pass the input sequences and masks through the encoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "\n",
    "# Pass the encoder's output and input sequences through the decoder\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "\n",
    "# Print input sequences for verification\n",
    "print(\"Input sequences:\\n\", input_sequence)\n",
    "\n",
    "# Print encoder output shape\n",
    "print(\"Encoder output shape:\", encoder_output.shape)\n",
    "\n",
    "# Print encoder output for inspection\n",
    "print(\"Encoder output:\\n\", encoder_output)\n",
    "\n",
    "# Print the shape of the decoder's output to verify the dimensions\n",
    "print(\"Batch's output shape:\", decoder_output.shape)\n",
    "\n",
    "# Print decoder output for inspection\n",
    "print(\"Decoder output:\\n\", decoder_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a14907-7d27-4f0e-b342-4dfcfe098d86",
   "metadata": {},
   "source": [
    "# More Complex Problem Solving Using Pre-Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927ee0b-1c37-41d3-aa57-428127d73f20",
   "metadata": {},
   "source": [
    "## Classifying two movie opinions\n",
    "\n",
    "We have seen how to pass one example sequence to a pre-trained text classification LLM for inference. In this exercise you will practice passing two example sequences simultaneously, describing two rather opposite opinions of a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0881cb-4bd5-4aaa-9f56-d30b0b492962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d3a5800-6931-4345-b253-2f28e5789602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for \"The best movie I've ever watched!\": 1\n",
      "Predicted class for \"What an awful movie. I regret watching it.\": 0\n"
     ]
    }
   ],
   "source": [
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize the input text and convert them to tensor format for the model\n",
    "# The `return_tensors=\"pt\"` argument specifies that the output should be in PyTorch tensor format\n",
    "# `padding=True` ensures that sequences are padded to the same length for batch processing\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Pass the tokenized inputs to the model for inference\n",
    "# The model returns output logits, which are the raw predictions before applying softmax\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract the logits from the model's output\n",
    "logits = outputs.logits\n",
    "\n",
    "# Use argmax to determine the predicted class for each input\n",
    "# The `dim=1` argument specifies that we want to find the index of the maximum logit along the class dimension\n",
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "# Loop through each input text and its predicted class to display the results\n",
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")  # Print the predicted class for each input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df80d95-4072-4c0a-b332-0324144d86a6",
   "metadata": {},
   "source": [
    "## Text-Generation\n",
    "\n",
    "In this exercise, we’ll explore how to generate text using the pre-trained GPT-2 model. We will start by loading a dataset from Stanford NLP to extract a sample prompt. After that, we’ll use the GPT-2 tokenizer to prepare this prompt for the model. Once our input is ready, we'll generate some text based on the prompt and decode the output back into a readable format. Let’s dive in and see how it works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be9a33bb-63c5-4b9f-9a3d-57c435e46f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d6ff9a8cd441c09d4c1739278258c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164187ccf7be415e9c967ff3e32c9f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a462c69814449aa640368e7bbe027c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an interview right before receiving the 2013 Nobel prize  for his work on the \"The Great Gatsby\" , he said: \"I think that the best way to understand the world is to look at the world in a different way.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"stanfordnlp/shp\", \"default\")\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# Prepare prompt from dataset\n",
    "prompt = train_data[0][\"history\"][:60]\n",
    "\n",
    "# Load the tokenizer and pre-trained model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad_token_id to eos_token_id to avoid padding error\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize input and get input_ids and attention_mask\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Generate text, using pad_token_id and attention_mask from tokenizer\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    attention_mask=inputs[\"attention_mask\"]\n",
    ")\n",
    "\n",
    "# Decode generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e49901-4595-4c64-8d4e-6327416232a9",
   "metadata": {},
   "source": [
    "## Text Summarizer\n",
    "In this exercise let's summarize a random Facebook CNN text using Text Summarizer LMM techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d172c0-f3cc-4147-96da-38c37340ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a BART model for summarization\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc902208-375b-49ca-9ce4-c7c9d51ea622",
   "metadata": {},
   "source": [
    "ATENTION SOMETIMES THE FEATURES ARE \"text\" AND SOMETIMES ARE \"history\" I DON'T UNDERSTAND WHY. IF GIVES YOU ERROR CHANGE IN THE CODE OR RESTART YOUR KERNEL. (RESULTS WHEN FEATURE IS \"text\" ARE WAY BETTER THAN WHEN FEATURE IS \"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56ef8731-5423-454e-ae74-bbd4ac3155e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['post_id', 'domain', 'upvote_ratio', 'history', 'c_root_id_A', 'c_root_id_B', 'created_at_utc_A', 'created_at_utc_B', 'score_A', 'score_B', 'human_ref_A', 'human_ref_B', 'labels', 'seconds_difference', 'score_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Search in the column names for the feature that contains the text we want to summarize\n",
    "# In this case we will be sumarizing a \"text\" from an example\n",
    "print(f\"Feature names: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2d25d3b-2d9d-4858-bd9e-dc65f3e7ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text (first 1000 characters): \n",
      " Can I get in trouble for giving my neighbor his leaves back? I live in Louisiana and every year around this time I have to deal with this crap. My neighborhood is small and quiet and we generally love it, except that my neighbor is an asshole. My yard has no big trees in it and the small Crepe Myrtles I have make very little mess. My neighbor, on the other hand has a huge Live Oak and some other tree that has enormous fricken leaves.  First off, he doesn’t take good care of his own yard so the leaves just pile up and eventually come into my yard, which is something I have always dealt with without making a big deal about it until a month or so ago. I came home a little early to eat lunch and he happened to be blowing off his driveway, straight into my yard. I didn’t say anything but I waited until he went inside and got my backpack blower(I own lawn service) and gave him his leaves back.   Since then we have had a sort of silent war of leaves and yesterday I went outside and there were\n",
      "\n",
      "Generated Summary: \n",
      " I live in Louisiana and every year around this time I have to deal with this crap. My neighbor has a huge Live Oak and some other tree that has enormous fricken leaves. He doesn’t take good care of his own yard so the leaves just pile up and eventually come into my yard. I waited until he went inside and got my backpack blower(I own lawn service) and gave him his leaves back.\n"
     ]
    }
   ],
   "source": [
    "# Define a minimum length for the review (e.g., 200 characters)\n",
    "min_length = 200\n",
    "\n",
    "# Filter for longer reviews\n",
    "long_reviews = dataset['train'].filter(lambda example: len(example['history']) >= min_length)\n",
    "\n",
    "# Access an example\n",
    "example = long_reviews[-5]['history']\n",
    "\n",
    "# Generate a summary using the previous model\n",
    "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "summary_ids = model.generate(input_ids, max_length=200, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal Text (first 1000 characters): \\n\", example[:1000])\n",
    "print(\"\\nGenerated Summary: \\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab392d-cbaf-47f9-aa1d-72d9017ab31c",
   "metadata": {},
   "source": [
    "Let's evaluate our model! For text summarization we will use ROUGE as a metric. ROUGE measures the overlap between the generated summaries and reference summaries by calculating the number of matching n-grams (e.g., unigrams, bigrams) and the longest common subsequences. Higher ROUGE scores indicate better similarity and relevance of the generated text to the reference content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "965a867b-bd64-4f53-a894-e028e68a5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': np.float64(0.3173277661795407), 'rouge2': np.float64(0.30607966457023067), 'rougeL': np.float64(0.3173277661795407), 'rougeLsum': np.float64(0.3173277661795407)}\n"
     ]
    }
   ],
   "source": [
    "# Load the ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Assuming you have the original text and generated summary\n",
    "results = rouge.compute(predictions=[summary], references=[example])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7bcc8-f7f9-4e80-8d74-70e9df5c9857",
   "metadata": {},
   "source": [
    "#### ROUGE Score Ranges\n",
    "\n",
    "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores can be interpreted based on the following ranges:\n",
    "\n",
    "##### ROUGE-1\n",
    "- **0.0 - 0.2**: Poor performance; little to no overlap with the reference.\n",
    "- **0.2 - 0.4**: Fair performance; some overlap, but significant gaps.\n",
    "- **0.4 - 0.6**: Good performance; reasonable overlap indicating solid content retention.\n",
    "- **0.6 - 0.8**: Very good performance; high overlap suggesting effective summarization.\n",
    "- **0.8 - 1.0**: Excellent performance; strong overlap with the reference, indicating nearly identical word usage.\n",
    "\n",
    "##### ROUGE-2\n",
    "- **0.0 - 0.1**: Poor performance; minimal bigram overlap.\n",
    "- **0.1 - 0.3**: Fair performance; some bigram matching, but lacks depth.\n",
    "- **0.3 - 0.5**: Good performance; adequate bigram retention showing content quality.\n",
    "- **0.5 - 0.7**: Very good performance; substantial bigram overlap reflecting coherent summarization.\n",
    "- **0.7 - 1.0**: Excellent performance; high bigram match indicating detailed and nuanced summaries.\n",
    "\n",
    "##### ROUGE-L\n",
    "- **0.0 - 0.2**: Poor performance; weak sentence structure preservation.\n",
    "- **0.2 - 0.4**: Fair performance; some preservation of important sequences, but inconsistent.\n",
    "- **0.4 - 0.6**: Good performance; maintains significant structure and coherence in summarization.\n",
    "- **0.6 - 0.8**: Very good performance; effectively preserves order and important sequences.\n",
    "- **0.8 - 1.0**: Excellent performance; exceptional retention of the original structure and meaning.\n",
    "\n",
    "##### ROUGE-Lsum\n",
    "- **0.0 - 0.2**: Poor performance; minimal content retention across the summary.\n",
    "- **0.2 - 0.4**: Fair performance; some key content retained, but lacks consistency.\n",
    "- **0.4 - 0.6**: Good performance; maintains a solid grasp of the main ideas in the summary.\n",
    "- **0.6 - 0.8**: Very good performance; strong retention of content and coherent summarization.\n",
    "- **0.8 - 1.0**: Excellent performance; exceptional summarization that captures the essence of the content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72928ca-d27b-4bd2-813b-5d7d2e9c0403",
   "metadata": {},
   "source": [
    "# Text Translation\n",
    "Some experimentation with text translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ea2cfcf-6f21-4c36-a8f8-9b6e36d47b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Hello | Spanish: Hola.\n",
      "English: Thank you | Spanish: Gracias.\n",
      "English: How are you? | Spanish: ¿Cómo estás?\n",
      "English: Sorry | Spanish: Lo siento.\n",
      "English: Goodbye | Spanish: Adiós.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "\n",
    "# Load the tokenizer and the model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
    "\n",
    "# Encode the inputs, generate translations, decode, and print them\n",
    "\n",
    "for english_input in english_inputs:\n",
    "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
    "    translated_ids = model.generate(input_ids)\n",
    "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f320f6b5-806e-4494-9e47-58934eee7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Hello, how are you today? | Spanish: Hola, ¿cómo estás hoy?\n",
      "English: Thank you for your help! | Spanish: ¡Gracias por su ayuda!\n",
      "English: This is an advanced translation model example. | Spanish: Este es un ejemplo avanzado de modelo de traducción.\n",
      "English: Sorry for the inconvenience, please try again. | Spanish: Siento las molestias, por favor, inténtalo de nuevo.\n",
      "English: Goodbye! Have a great day ahead. | Spanish: Que tengas un gran día por delante.\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# List of sentences to translate\n",
    "english_inputs = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Thank you for your help!\",\n",
    "    \"This is an advanced translation model example.\",\n",
    "    \"Sorry for the inconvenience, please try again.\",\n",
    "    \"Goodbye! Have a great day ahead.\"\n",
    "]\n",
    "\n",
    "# Translation function with advanced features\n",
    "def translate_batch(texts, max_length=50, num_beams=5):\n",
    "    # Encode batch of sentences and translate\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    # Decode translations\n",
    "    translations = [tokenizer.decode(ids, skip_special_tokens=True) for ids in translated_ids]\n",
    "    return translations\n",
    "\n",
    "# Translate and display\n",
    "translated_texts = translate_batch(english_inputs)\n",
    "for eng, esp in zip(english_inputs, translated_texts):\n",
    "    print(f\"English: {eng} | Spanish: {esp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716629b5-4bbc-4a8c-b391-a2f70938f853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abc474-5f8f-4b97-a935-9ba859a439a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97afce-09fc-4c0f-ad32-6171fb3e07c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
